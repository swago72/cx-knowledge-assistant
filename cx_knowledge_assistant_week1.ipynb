{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnI/Oz92bdFi0JKfe+wa2T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swago72/cx-knowledge-assistant/blob/main/cx_knowledge_assistant_week1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-68YTiBsJgYx"
      },
      "outputs": [],
      "source": [
        "!pip install langchain chromadb sentence-transformers beautifulsoup4 lxml -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-text-splitters -q"
      ],
      "metadata": {
        "id": "dcEBwVFPLtOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Select all 13 HTML files from your raw folder\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(f\"\\n✓ {len(uploaded)} files uploaded:\")\n",
        "for fname in uploaded.keys():\n",
        "    print(f\"  - {fname}\")"
      ],
      "metadata": {
        "id": "boiEJgERLD4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "\n",
        "# ── STEP 1: Load + clean HTML ───────────────────────────────────\n",
        "\n",
        "def load_html_files(uploaded_files):\n",
        "    documents = []\n",
        "    for filename, content in uploaded_files.items():\n",
        "        raw_html = content.decode(\"utf-8\", errors=\"ignore\")\n",
        "        soup = BeautifulSoup(raw_html, \"lxml\")\n",
        "\n",
        "        # Strip noise\n",
        "        for tag in soup([\"script\", \"style\", \"nav\", \"header\",\n",
        "                          \"footer\", \"aside\", \"noscript\", \"iframe\"]):\n",
        "            tag.decompose()\n",
        "\n",
        "        clean_text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "        lines = [l for l in clean_text.splitlines() if l.strip()]\n",
        "        clean_text = \"\\n\".join(lines)\n",
        "\n",
        "        if len(clean_text) < 100:\n",
        "            print(f\"  ⚠ Skipping {filename} — too short\")\n",
        "            continue\n",
        "\n",
        "        # Create a short clean name for citations\n",
        "        short_name = filename.replace(\" - Google Play Help.html\", \"\")\n",
        "\n",
        "        documents.append({\n",
        "            \"content\":    clean_text,\n",
        "            \"source\":     filename,\n",
        "            \"short_name\": short_name,\n",
        "            \"chars\":      len(clean_text)\n",
        "        })\n",
        "        print(f\"  ✓ {short_name} ({len(clean_text):,} chars)\")\n",
        "\n",
        "    print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
        "    return documents\n",
        "\n",
        "\n",
        "# ── STEP 2: Chunk ───────────────────────────────────────────────\n",
        "\n",
        "def chunk_documents(documents):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=50,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
        "    )\n",
        "\n",
        "    chunks = []\n",
        "    for doc in documents:\n",
        "        texts = splitter.split_text(doc[\"content\"])\n",
        "        for i, text in enumerate(texts):\n",
        "            # Create a safe ID — remove spaces and special chars\n",
        "            safe_id = doc[\"source\"].replace(\" \", \"_\") \\\n",
        "                                   .replace(\"-\", \"_\") \\\n",
        "                                   .replace(\".\", \"_\") \\\n",
        "                                   [:80]\n",
        "            chunks.append({\n",
        "                \"text\":       text,\n",
        "                \"source\":     doc[\"source\"],\n",
        "                \"short_name\": doc[\"short_name\"],\n",
        "                \"chunk_id\":   f\"{safe_id}_chunk_{i}\",\n",
        "                \"chunk_num\":  i,\n",
        "                \"total\":      len(texts)\n",
        "            })\n",
        "\n",
        "    print(f\"Total chunks created: {len(chunks)}\")\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# ── STEP 3: Embed + store ───────────────────────────────────────\n",
        "\n",
        "def embed_and_store(chunks, collection_name=\"cx_knowledge_base\"):\n",
        "    print(\"Loading HuggingFace model (downloads once ~80MB)...\")\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "    print(\"✓ Model ready\\n\")\n",
        "\n",
        "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "    collection = client.get_or_create_collection(\n",
        "        name=collection_name,\n",
        "        metadata={\"hnsw:space\": \"cosine\"}\n",
        "    )\n",
        "\n",
        "    existing_ids = set(collection.get()[\"ids\"])\n",
        "    new_chunks = [c for c in chunks\n",
        "                  if c[\"chunk_id\"] not in existing_ids]\n",
        "\n",
        "    if not new_chunks:\n",
        "        print(\"Already embedded. Nothing to add.\")\n",
        "        return collection, model\n",
        "\n",
        "    print(f\"Embedding {len(new_chunks)} chunks...\")\n",
        "    batch_size = 50\n",
        "\n",
        "    for i in range(0, len(new_chunks), batch_size):\n",
        "        batch  = new_chunks[i:i + batch_size]\n",
        "        texts  = [c[\"text\"] for c in batch]\n",
        "        embeds = model.encode(texts, show_progress_bar=False).tolist()\n",
        "\n",
        "        collection.add(\n",
        "            embeddings=embeds,\n",
        "            documents=[c[\"text\"] for c in batch],\n",
        "            metadatas=[{\n",
        "                \"source\":     c[\"source\"],\n",
        "                \"short_name\": c[\"short_name\"],\n",
        "                \"chunk_num\":  c[\"chunk_num\"]\n",
        "            } for c in batch],\n",
        "            ids=[c[\"chunk_id\"] for c in batch]\n",
        "        )\n",
        "        done = min(i + batch_size, len(new_chunks))\n",
        "        print(f\"  Stored {done}/{len(new_chunks)}\")\n",
        "\n",
        "    print(f\"\\n✓ ChromaDB contains {collection.count()} total chunks\")\n",
        "    return collection, model\n",
        "\n",
        "\n",
        "# ── RUN ─────────────────────────────────────────────────────────\n",
        "\n",
        "print(\"=\" * 55)\n",
        "print(\"  CX Knowledge Assistant — Ingestion Pipeline\")\n",
        "print(\"=\" * 55 + \"\\n\")\n",
        "\n",
        "print(\"STEP 1: Loading HTML files...\")\n",
        "documents = load_html_files(uploaded)\n",
        "\n",
        "print(\"\\nSTEP 2: Chunking...\")\n",
        "chunks = chunk_documents(documents)\n",
        "\n",
        "print(\"\\nSTEP 3: Embedding and storing in ChromaDB...\")\n",
        "collection, embed_model = embed_and_store(chunks)\n",
        "\n",
        "print(\"\\n✅ Done. Knowledge base is ready for Week 2.\")"
      ],
      "metadata": {
        "id": "M4kVI6fqLb3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_query(question, n=3):\n",
        "    embedding = embed_model.encode(question).tolist()\n",
        "    results = collection.query(\n",
        "        query_embeddings=[embedding],\n",
        "        n_results=n,\n",
        "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
        "    )\n",
        "\n",
        "    print(f\" ? '{question}'\\n\")\n",
        "    for i, (doc, meta, dist) in enumerate(zip(\n",
        "        results[\"documents\"][0],\n",
        "        results[\"metadatas\"][0],\n",
        "        results[\"distances\"][0]\n",
        "    )):\n",
        "        score = round((1 - dist) * 100, 1)\n",
        "        print(f\"  Result {i+1} — {meta['short_name']} \"\n",
        "              f\"({score}% match)\")\n",
        "        print(f\"  {doc[:180]}...\")\n",
        "        print()\n",
        "\n",
        "# Run all 5 — one per issue type\n",
        "test_query(\"How do I request a refund for an app?\")\n",
        "test_query(\"My subscription was charged after I cancelled it\")\n",
        "test_query(\"App is stuck on pending and won't download\")\n",
        "test_query(\"There's a charge on my account I don't recognize\")\n",
        "test_query(\"I was charged for an in-app purchase I didn't make\")"
      ],
      "metadata": {
        "id": "pgTuLL8oNey4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}